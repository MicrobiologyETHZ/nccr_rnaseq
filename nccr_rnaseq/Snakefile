from scripts.get_vars import *
import sys
import pandas as pd


print("loading configs.")
DATADIR = Path(config['dataDir'])
OUTDIR = Path(config['outDir'])

print(OUTDIR)
# Sample Name, Unit, forward reads, reverse reads
sampleInfo = pd.read_csv(config['samples'])

samples_to_merge = (sampleInfo.loc[sampleInfo.groupby('sample')
                    .unit.filter(lambda x: x.nunique() > 1).index]['sample']
                    .unique())

SAMPLES = pd.read_csv(config['samples'])['sample'].unique()

include: "rules/preprocessing.smk"
include: "rules/star_salmon.smk"
include: "rules/bwa_htseqcount.smk"
include: "rules/bowtie_featurecounts.smk"
include: "rules/salmon_metat.smk"
include: "rules/coptr.smk"
include: "rules/mvirs.smk"


mvir_files = [
    OUTDIR / f'mvirs_out/{sample}.mvirs_oprs.done' for sample in SAMPLES]


rule run_mvirs:
    input: mvir_files


if 'teAnn' in config.keys():
    include: "rules/te_counts.smk"
    count_te_input_files = [
        f'{OUTDIR}/tetranscripts/{sample}/{sample}.te.done' for sample in SAMPLES]
    rule count_te:
        input: count_te_input_files


"""
Preprocessing:

In the config file, can specify whether to run FastQC before preprocessing, after, both or not at all.
By default, will only run after preprocessing.

Preprocessing:
    - 

"""
if not config['fastqc']:
    preprocess_files = [
        OUTDIR/f'clean_reads/{sample}/{sample}.qc.done' for sample in SAMPLES]
    rule preprocess:
        input: preprocess_files
elif config['fastqc'] == 'after':
    rule preprocess:
        input: OUTDIR/'fastqc/after.multiqc.done'
elif config['fastqc'] == 'before':
    rule preprocess:
        input: OUTDIR/'fastqc/before.multiqc.done'
elif config['fastqc'] == 'both':
    rule preprocess:
        input: OUTDIR/'fastqc/both.multiqc.done'
else:
    print(f'{config["fastqc"]} is not a recognized option for fastqc')
    sys.exit(1)

# EUKARYOTIC RULES

rule salmon:
    input: [OUTDIR / f'salmon/{sample}_quant/quant.sf' for sample in SAMPLES]

rule star:
    input: OUTDIR/f'counts/{config["projectName"]}.merged.featureCounts.csv'

# if 'multimap' in config.keys() and config['multimap'] == True:
#     mergeFeatureCounts_files = [
#         OUTDIR/f'counts/{sample}/{sample}.count.mm.txt' for sample in SAMPLES]
#     rule mergeFeatureCounts:
#         input: mergeFeatureCounts_files
#         output: OUTDIR / \
#             f'counts/{config["projectName"]}.merged.featureCounts.csv'
#         params:
#             def qerrfile(wildcards): return OUTDIR / \
#                 f'logs/merge.featurecounts.qerr',

#             def qoutfile(wildcards): return OUTDIR / \
#                 f'logs/merge.featurecounts.qout',
#             outdir = OUTDIR/'counts',
#             threads = 32,
#             scratch = 6000,
#             mem = 8000,
#             time = 1400
#         log: OUTDIR/'logs/featurecounts.merge.log'
#         threads:
#             4
#         run:
#             pd_list = []
#             for f in input:
#                 df = pd.read_table(
#                     f, skiprows=[0], index_col=0).iloc[:, [4, 5]]
#                 df.columns = ["Length", Path(f).stem.split(".")[0]]
#                 pd_list.append(df)
#             fdf = pd.concat(pd_list, axis=1)
#             fdf = fdf.loc[:, ~fdf.columns.duplicated()]
#             fdf.to_csv(output[0])

# else:
#     mergeFeatureCounts_files = [
#         OUTDIR/f'counts/{sample}/{sample}.count.txt' for sample in SAMPLES]
#     rule mergeFeatureCounts:
#         input: mergeFeatureCounts_files
#         output: OUTDIR / \
#             f'counts/{config["projectName"]}.merged.featureCounts.csv'
#         params:
#             def qerrfile(wildcards): return OUTDIR / \
#                 f'logs/merge.featurecounts.qerr',

#             def qoutfile(wildcards): return OUTDIR / \
#                 f'logs/merge.featurecounts.qout',
#             outdir = OUTDIR/'counts',
#             threads = 32,
#             scratch = 6000,
#             mem = 8000,
#             time = 1400
#         log: OUTDIR/'logs/featurecounts.merge.log'
#         threads:
#             4
#         run:
#             pd_list = []
#             for f in input:
#                 df = pd.read_table(
#                     f, skiprows=[0], index_col=0).iloc[:, [4, 5]]
#                 df.columns = ["Length", Path(f).stem.split(".")[0]]
#                 pd_list.append(df)
#             fdf = pd.concat(pd_list, axis=1)
#             fdf = fdf.loc[:, ~fdf.columns.duplicated()]
#             fdf.to_csv(output[0])

# METAT RULES
rna_filter_files = [
    OUTDIR / f'rnasorted/{sample}.sortmerna.done' for sample in SAMPLES]
rule rna_filter:
    input: rna_filter_files

bowtie_files = [
    OUTDIR/f'bowtie_featurecounts/{sample}/{sample}.count.txt' for sample in SAMPLES]
rule bowtie:
    input: bowtie_files

htseq_count_files = [
    OUTDIR / f'htseqcount/{sample}/{sample}.htseqcount.txt' for sample in SAMPLES]
rule htseq_count:
    input: htseq_count_files

sushi_align_files = [
    OUTDIR/f'sushibam/{sample}/{sample}.shushicounts' for sample in SAMPLES]
rule sushi_align:
    input: sushi_align_files

salmon_metat_files = [
    OUTDIR/f"salmon_metat/{sample}_quant/quant.sf" for sample in SAMPLES]
rule salmon_metat:
    input: salmon_metat_files


# rule kallisto:
#     input: [OUTDIR/f'kallisto/{sample}.done' for sample in SAMPLES]


# rule profile:
#     input: [OUTDIR / f'motus/{sample}/{sample}.motus' for sample in SAMPLES]

# # SATDNA rules

# rule satdna:
#     input: [OUTDIR/f'kseek/{sample}.rep.total' for sample in SAMPLES]


rule run_coptr:
    input: OUTDIR/'coptr.done'
