from scripts.get_vars import *
import sys
import pandas as pd

DATADIR = Path(config['dataDir'])
OUTDIR = Path(config['outDir'])


# Sample Name, Unit, forward reads, reverse reads
sampleInfo = pd.read_csv(config['samples'])

samples_to_merge = (sampleInfo.loc[sampleInfo.groupby('sample')
                    .unit.filter(lambda x: x.nunique() > 1).index]['sample']
                    .unique())

SAMPLES = pd.read_csv(config['samples'])['sample'].unique()

include: "rules/preprocessing.smk"
include: "rules/rnaseq.smk"

"""
Preprocessing:

In the config file, can specify whether to run FastQC before preprocessing, after, both or not at all.
By default, will only run after preprocessing.

Preprocessing:
    - 

"""
if not config['fastqc']:
    rule preprocess:
        input: [OUTDIR/f'clean_reads/{sample}/{sample}.qc.done' for sample in SAMPLES]
elif config['fastqc'] == 'after':
    rule preprocess:
        input: OUTDIR/'fastqc/after.multiqc.done'
elif config['fastqc'] == 'before':
    rule preprocess:
        input: OUTDIR/'fastqc/before.multiqc.done'
elif config['fastqc'] == 'both':
    rule preprocess:
        input: OUTDIR/'fastqc/both.multiqc.done'
else:
    print(f'{config["fastqc"]} is not a recognized option for fastqc')
    sys.exit(1)


# rule star:
#     input: [OUTDIR/f'counts/{sample}/{sample}.count.txt' for sample in SAMPLES]


rule star:
    input: OUTDIR/f'counts/{config["projectName"]}.merged.featureCounts.csv'

rule mergeFeatureCounts:
    input: [OUTDIR/f'counts/{sample}/{sample}.count.txt' for sample in SAMPLES]
    output: OUTDIR/f'counts/{config["projectName"]}.merged.featureCounts.csv'
    params:
        qerrfile = lambda wildcards: OUTDIR/f'logs/merge.featurecounts.qerr',
        qoutfile = lambda wildcards: OUTDIR/f'logs/merge.featurecounts.qout',
        outdir = OUTDIR/'counts',
        threads = 32,
        scratch = 6000,
        mem = 8000,
        time = 1400
    log: OUTDIR/'logs/featurecounts.merge.log'
    threads:
        4
    run:
        pd_list = []
        for f in input:
            df = pd.read_table(f, skiprows=[0], index_col=0).iloc[:,[4,5]]
            df.columns = ["Length", Path(f).stem.split(".")[0]]
            pd_list.append(df)
        fdf = pd.concat(pd_list, axis=1)
        fdf = fdf.loc[:, ~fdf.columns.duplicated()]
        fdf.to_csv(output[0])


# rule kallisto:
#     input: [OUTDIR/f'kallisto/{sample}.done' for sample in SAMPLES]

rule salmon:
    input: [OUTDIR / f'salmon/{sample}_quant/quant.sf' for sample in SAMPLES]